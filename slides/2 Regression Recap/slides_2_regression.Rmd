---
title: "[ECON42720 Causal Inference and Policy Evaluation](https://benelsner82.github.io/causalinfUCD/)"
subtitle: "2 Regression Recap"
author: "Ben Elsner (UCD)"
output:
  beamer_presentation:
    includes:
      in_header: "../Templates/template_metrics.tex"
classoption: "aspectratio=169, handout" 


--- 

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(ggplot2)
library(wooldridge)
library(stargazer)
library(foreign)

set.seed(1234)
graphdir <- "../../../causalinf_phd/Graphs/"


# Define a theme for the graphs
graphtheme <-  theme(
    panel.grid.major = element_blank(),  # Remove major gridlines
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    axis.line = element_line(color = "black"),  # Add black axis lines
    axis.text.x = element_text(color = "black", size = 16),  # Customize x-axis text
    axis.text.y = element_text(color = "black", size = 16),  # Customize y-axis text
  )

# Graph parameters
basesize <- 25
```

## About this Lecture

\brf{Linear regression} is by far the most important \brf{estimation technique in causal inference}
\vfill

Yes, I know, \br{machine learning is all the rage these days}

- But a linear approximation is often the best we can do
- It's already hard enough to get the linear approximation right
- Fancy techniques are not always better


## Resources

The material behind these slides can be found in any good econometrics textbook. For introductory econometrics, I recommend

- Wooldridge, J. Introductory Econometrics: A Modern Approach. 7th Edition. South-Western College Publishing, 2019.
- Stock, J. and M. Watson. Introduction to Econometrics. 3rd Edition. Pearson, 2017.



## Regression recap

\br{Why do we use linear regression?}

- What we want to approximate: the conditional expectation function (CEF)


\vfill

\br{How does regression work?}

- Interpretation of coefficients with and without controls
- Estimation with OLS and Inference


## Undergrad Recap: Goal of Linear Regression

Quantify the \brf{expected effect} of a **one unit change in $X$ on $Y$**

- If $X$ goes up by one unit, by how many units does $Y$ go up or down?
- **Causal interpretation:** If we/nature/an experimenter changes $X$ by one unit, what is the expected effect on $Y$?
\vfill

This \brf{effect} is equivalent to the \brf{slope} coefficient $\beta_1$ in a \brf{linear regression model}

\begin{equation*}
Y=\beta_0 + \beta_1 X + u
\end{equation*}



## Linear Regression

Regression analysis means that we \brf{fit a straight line} through $(X,Y)$ data points

```{r, out.width = '45%', echo=FALSE, fig.align="center"}
  nr <- 100
  beta0 <- -3000
  beta1 <- 700
  iq <- rnorm(n=nr, mean=100, sd=15)
  u <- 15000*rnorm(n=nr, mean=0, sd=1)
  y <- (beta0 + beta1*iq + u)/1000
  data <- data.frame(iq, u, y)
  par(mar=c(5,6,4,1)+.1)
  plot(iq, y, 
       xlab="IQ",
       ylab="Earnings in 1000EUR",
       cex.lab=2, cex.axis=2, cex=2)
  abline(lm(y~iq),
         col="red",
         lwd=4)
```

In this example, the **regression line** is $\text{Earnings}=-3000 + 700 \mbox{ IQ}$

- An increase in the IQ by one point increases earnings on average by 700 EUR



## What we are looking for: the conditional expectation function

In undergraduate econometrics, you probably learned about a \brf{population regression model}

- the idea is that there is a **true relationship** between $X$ and $Y$ that we want to estimate
- we have a **sample** of $n$ observations from this population and estimate $\widehat{\beta}_0, \widehat{\beta}_1$ using OLS

\vfill

But is the \br{population regression model (PRM)} really what we are looking for?

- Yes and no. The PRM is an approximation of our object of interest
- This object is called the \brf{conditional expectation function} (CEF) 


## Ingredients: Random Variables

\brf{Random variables} are variables that take on different values with a certain probability

- $x$ is a random variable that takes on values $x_1, x_2, \ldots, x_n$ with probabilities $f(x_1), f(x_2), \ldots, f(x_n)$

\vfill

\brf{Expected value}: the average value of a random variable


\begin{align*}
  E(x) & = x_1f(x_1)+x_2f(x_2)+\dots+x_kf(x_n) \\
  & = \sum_{j=1}^n x_jf(x_j)              
\end{align*}


## Expected Value: Example

$x \in \{-1, 0, 2\}$ with probabilities $f(-1)=0.3, f(0)=0.3, f(2)=0.4$. The expected value of $X$ is


\begin{align*}
  E(x) & = (-1)(0.3)+(0)(0.3)+(2)(0.4) \\
  & = 0.5                         
\end{align*}



## Notation

We denote **random variables with lower case letters** $x, y$. 

\vfill

Typically, we **do not use indices when we talk about the population**. For example, the linear relationship between $x$ and $y$ in the population is 

\begin{equation*}
y=\beta_0+\beta_1x+u
\end{equation*}

\vfill
Realisations of a random variable are denoted with **lower case letters with indices $x_i$** with $i=1,\dots, n$. We also use indices when 

- Talking about **relationships in the sample**
- Talking about particular realisations of a random variable: $x_i=x$, for example $x_i=female$ or $x_i=5$


\vfill
This can be confusing at the start but you'll get used to it!



## The Conditional Expectation Function

We are interested in \brf{explaining the relationship between $x$ and $y$} in the population
\vfill
A useful concept in this regard is the \brf{Conditional Expecation Function (CEF)}: $E(y_i|x_i)$

- what is the **population average of $y_i$ for a given value of $x_i$**
- i.e. what if $x_i$ takes value $x$?


\vfill
\brf{Example}: $x$ is a dummy that equals one if a person is female and zero otherwise. $y$ is earnings.

\vfill
The **CEF can take on two values**: 

- Average earnings of women $E(y_i|x_i=1)$
- Average earnings of men/other $E(y_i|x_i=0)$ 


## A Continuous CEF

Education vs earnings (from Angrist & Pischke, MHE)

```{r, out.width = '60%', echo=FALSE, fig.align="center"}
knitr::include_graphics(paste0(graphdir, "CEF_schooling.PNG"))
```

At every level of completed education, we have a different expected value of earnings

- At each value $x_i$ we have a distribution of $y_i$ 
- and the CEF comprises the averages of this distribution



## Regression is a Linear Approximation of the CEF

The \brf{population regression model} (PRM) is a linear approximation of the CEF

$$y=\beta_0+\beta_1x+u$$

- Our **ultimate goal** is to know the **CEF**
- But: with a linear regression, we can estimate the parameters of the PRM
- I.e. we **cannot estimate the CEF directly**

\vfill

Why a \brf{linear approximation is useful}: 

- We typically have small sample sizes, so approximating a non-linear function is difficult
- We are often interested in marginal effects, so a linear approximation is often sufficient



## PRM: Linear Approximation of the CEF

```{r, out.width = '60%', echo=FALSE, fig.align="center"}
knitr::include_graphics(paste0(graphdir, "CEF_schooling2.PNG"))
```

The dashed line is the Population regression model $y=\beta_0 + \beta_1 x + u$. The solid line is the CEF $E(y|x)$. 

\vfill
It can be shown that the PRM is the \brf{best linear approximation of the CEF} (see Angrist & Pischke, MHE, ch. 3). 



## CEF and PRM: what's all this about?`

The \brf{CEF is the object of interest} in (most of) econometrics

- The PRM $y= \beta_0 + \beta_1x + u$ is a **linear approximation** of the CEF. 
- But it is an approximation, so it can be wrong.
- With data, we can draw inference on the PRM but not on the CEF

\vfill

\brf{What does this mean for the empirical analysis?}

- We need to think about the **relationship between $x$ and $y$** in the population
- Linear approximations are more innocuous when we consider small changes in $x$

## The Sample Regression Function

Now suppose we have a \brf{sample of size $n$} that was \brf{randomly sampled from the population}, $(y_1, x_1), \dots, (y_n, x_n)$
\vfill
The \brf{sample regression function} is 
\begin{equation}
\widehat{y_i}=\widehat{\beta_0} + \widehat{\beta_1} x_i
\label{eq13}
\end{equation}

\vfill

We can estimate the parameters $\widehat{\beta_0}$ and $\widehat{\beta_1}$ with Ordinary Least Squares (OLS)



## OLS: Intuition

Let's start with some data points


```{r, out.width = '65%', echo=FALSE, fig.align="center"}
data <- data.frame(
  x = 1:10,
  y = 1 * 1:10 + 3*rnorm(10)
)

# Fit a linear model to the data
model <- lm(y ~ x, data = data)

# Add the fitted values and residuals to the data frame
data$fitted <- predict(model)
data$residuals <- residuals(model)

beta_0 <- coef(model)[1] + 4  # Adjust the intercept
beta_1 <- coef(model)[2] -1.2  # Adjust the slope

# Calculate the new fitted values based on the adjusted coefficients
data$fitted2 <- beta_0 + beta_1 * data$x
data$residuals2 <- data$y - data$fitted2

ggplot(data, aes(x = x, y = y)) +
  geom_point(size=5) + 
  theme_minimal(base_size=basesize) + 
  graphtheme
```

## OLS: Intuition

Goal: fit a regression line through those points

```{r, out.width = '65%', echo=FALSE, fig.align="center"}


ggplot(data, aes(x = x, y = y)) +
  geom_point(size=5) + 
  theme_minimal(base_size=basesize) + 
  geom_line(aes(y = fitted), color = "black", size=2.5) +  # Plot the fitted line
  graphtheme
```


## OLS: Intuition

The key ingredient of OLS are the residuals $\widehat{u}_i=y_i-\widehat{b_0}-\widehat{b_1}x_i$

```{r, out.width = '65%', echo=FALSE, fig.align="center"}
ggplot(data, aes(x = x, y = y)) +
  geom_point(size=5) +  # Plot the actual points
  geom_segment(aes(xend = x, yend = fitted), arrow = arrow(type = "closed", length = unit(0.15, "inches")), color = "red", size = 0.5) +  # Draw arrows from points to the fitted line
  geom_line(aes(y = fitted), color = "black", size=2.5) +  # Plot the fitted line
  theme_minimal(base_size=basesize) + 
  graphtheme
```

## OLS: Intuition

Now consider the square of each residual

```{r, out.width = '65%', echo=FALSE, fig.align="center"}
ggplot(data, aes(x = x, y = y)) +
  geom_point(size = 5) +  # Plot the actual points
  geom_rect(aes(xmin = x, xmax = x + residuals, ymin = pmin(y, fitted), ymax = pmax(y, fitted)),
            fill = "red", color = "red", alpha = 0.5) +  # Draw red squares
  geom_line(aes(y = fitted), color = "black", size = 2.5) +  # Plot the fitted line
  theme_minimal(base_size = 25) +
  graphtheme
```


## OLS: Intuition

Let's consider a different regression line: the squares are much larger!

```{r, out.width = '65%', echo=FALSE, fig.align="center"}
ggplot(data, aes(x = x, y = y)) +
  geom_point(size = 5) +  # Plot the actual points
  geom_rect(aes(xmin = x, xmax = x + residuals2, ymin = pmin(y, fitted2), ymax = pmax(y, fitted2)),
            fill = "red", color = "red", alpha = 0.5) +  # Draw red squares
  geom_line(aes(y = fitted2), color = "black", size = 2.5) +  # Plot the fitted line
  theme_minimal(base_size = 25) +
  graphtheme
```

## OLS: Intuition

OLS \brf{minimizes the average size of these squares}

\vfill

It \br{minimizes the sum of squared residuals (SSR)}

\vfill

The result is the \brf{best-fitting line} that describes the relationship between $x$ and $y$ in the sample



## OLS: Data Example

Let's look at the relationship between education and wages with data from the U.S.

```{r, out.width = '65%', echo=FALSE, fig.align="center"}
df <- data("wage1")
p <- ggplot(wage1, aes(x = educ, y = wage)) +
  geom_point(size = 5) +  # Plot the actual points
  geom_smooth(method = "lm", se = FALSE, color = "red", size = 2) +  # Add a regression line
  labs(x = "Education", y = "Hourly wage") +  # Label the axes
  theme_minimal(base_size = basesize) + 
  graphtheme

print(p)

```

## Regression Output: Interpretation

\tiny
```{r, results='asis', out.width='65%', echo=FALSE}
model <- lm(wage ~ educ, data = wage1)
stargazer(model, header=FALSE, type='latex', title="Effect of Education on Wages")
```
\normalsize

A 1-year increase in education is associated with a 0.54 USD increase in hourly wages




## OLS: The Math

The Ordinary Least Squares (OLS) estimators $\widehat{\beta_0}$ and $\widehat{\beta_1}$ are derived through the minimization problem
\begin{equation}
(\widehat{\beta_0}, \widehat{\beta_1})=\displaystyle\argmin_{\widehat{b_0}, \widehat{b_1}} \displaystyle\sum_{i=1}^n[(y_i-\widehat{b_0}-\widehat{b_1}x_i)^2]
\label{eq14}
\end{equation}
\vfill
The sample means $\bar{y}=\frac{1}{n}\displaystyle\sum_{i=1}^n y_i$ and $\bar{x}=\frac{1}{n}\displaystyle\sum_{i=1}^n x_i$ are the sample analogs of the population means $E(y_i)$ and $E(x_i)$



## OLS: The Math
The \brf{residuals of the regression} are defined as $\widehat{u_i}=y_i-\widehat{\beta_0} + \widehat{\beta_1} x_i$. 
\vfill
When we ``run an OLS regression'', we \brf{minimize the sum of squared residuals (SSR)}, $\displaystyle\sum_{i=1}^n \widehat{u_i}^2$ and obtain values for $\widehat{\beta_0}$ and $\widehat{\beta_1}$
\vfill
\pause
Solving the minimization problem (\ref{eq14}) yields the \brf{estimators}
\begin{eqnarray}
\widehat{\beta_1}&=&\frac{\frac{1}{n}\sum_{i=1}^n (y_i - \bar{y})(x_i-\bar{x})}{\frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})^2}=\frac{\widehat{Cov(y_i, x_i)}}{\widehat{V(x_i)}}\\
\widehat{\beta_0}&=&\bar{y}-\widehat{\beta_1}\bar{x} 
\nonumber
\label{eq15}
\end{eqnarray}



## The Sampling Distribution of the OLS Estimator

To \brf{draw inference about the population}, we need to know the \brf{sampling distribution of the OLS estimator}

\vfill

What we want to know: 

- When will $\widehat{\beta}_1$ be \textbf{unbiased?}
- What is its \textbf{variance?}

\vfill

To answer these questions, we need to make some \brf{assumptions about the sample and population} 

1. Population model is linear in parameters
2. Sample is randomly drawn from the population
3. Variation in $x$
4. **Zero conditional mean assumption** (ZCM)


## OLS Assumptions

The four \brf{OLS assumptions must be fulfilled} for the OLS estimator to be \brf{unbiased and consistent}

\vfill

\brf{Unbiasedness}: $E(\widehat{\beta}_1)=\beta_1$

- across many random samples, the estimator gets it right on average

\vfill
\brf{Consistency}: $\widehat{\beta}_1 \xrightarrow[]{p} \beta_1$ as $n\rightarrow \infty$

- if the sample size increases, the estimator converges to the true value
- this is a consequence of the Law of Large Numbers (LLN)
- As $n$ gets larger, the sample becomes more representative of the population




## The Zero Conditional Mean (ZCM) Assumption

The \brf{ZCM assumption is the most important assumption} in this module

- It is **not testable with the data** at hand
- It rarely holds in practice (except in randomised experiments)
- Causal inference techniques exploit scenarios where ZCM holds approximately

\vfill

\brf{Other names for the ZCM assumption}:

- **Conditional independence assumption (CIA)**
- **Exogeneity assumption**


## The Zero Conditional Mean (ZCM) Assumption

Consider the \brf{population model}

$$y=\beta_0+\beta_1 x + u$$

The \brf{ZCM assumption} states that the \br{conditional mean of the error term is zero}

$$E(u|x)=E(u)=0$$
\vfill
What does this mean?

- The error term is **not systematically related** (speak: uncorrelated) with $x$
- At any level of $x$, the average value of $u$ is zero


## ZCM Assumption: Example

Does \brf{higher education (causally) increase earnings?}

$$wage_i=\beta_0+\beta_1 education_i + u_i$$
\brf{What is the error term $u_i$ here?}

- Any determinant of a person's wage that is \br{not education} 
- E.g., innate ability, motivation, personality, etc.

## ZCM Assumption: Example
**Say $u_i$ includes ability**. According to the ZCM assumption, the following must hold:

$$E(\text{ability}\mid \text{education}=8)=E(\text{ability}\mid \text{education}=12)=E(\text{ability}\mid \text{education}=16)$$
\vfill
So it \brf{must hold that}:

- the average ability of people with 8 years of education is the same as
- the average ability of people with 12 years of education and
- the average ability of people with 16 years of education

\vfill
This is hardly plausible $\Rightarrow$ \brf{ZCM assumption is violated}


## What if ZCM is Violated?

The OLS estimator of $\beta_1$ is \brf{biased and inconsistent}

\vfill
\brf{Bias}: $E(\widehat{\beta_1}) \neq \beta_1$

- The expected value of the OLS estimator is not equal to the true value of $\beta_1$
- Across many samples, the estimates are systematically too big or too small

\vfill
\brf{Inconsistency}: $\widehat{\beta_1}$ does not converge to $\beta_1$ as $n\rightarrow \infty$

- Even if the sample size is very large, the OLS estimator does not converge to the true value of $\beta_1$


## How to think about Violations of ZCM

The variable $x$ is \brf{typically a choice}

- People choose how much education to get, how often they go to the gym, how much they save, who they want to date, etc
- Firms choose how much to invest, how many workers to hire, how much to pollute, etc.
- Governments choose how much to spend on education, how much to tax, etc.

\vfill


The \brf{choice} of $x$ is typically \brf{influenced by other factors} $u$

- Individual factors: ability, motivation, preferences, etc.
- Firm factors: technology, market conditions, etc.
- Government factors: ideology, political pressure, etc.

\vfill

\brf{Problem:} **$u$ affects $y$** not just through $x$ but also **through other paths** or directly

## How to think about Violations of ZCM

The error term $u$ includes \brf{one or more confounders}

```{r, out.width = '45%', echo=FALSE, fig.align="center"}
knitr::include_graphics(paste0(graphdir, "dag_confound1.png"))
```

Here $u$ includes a confounder $y$ directly and through $x$

\vfill
Example: $x$ is education, $y$ is earnings, $u$ is ability


## Omitted Variable Bias

Suppose the **true model** is $y=\beta_0 + \beta_1 x + \beta_2 s_1 + e$

\vfill

However, we **estimate the model** $y=\tilde\beta_0 + \tilde\beta_1 x + u$

\vfill
It can be shown that the \brf{OLS estimator is biased}


$$\tilde\beta_1 = \beta_1 + \underbrace{\beta_2 \frac{Cov(x,s_1)}{Var(x)}}_{OVB}$$


## So when does ZCM hold?

ZCM holds if $x$ is \brf{as good as randomly assigned} to individuals

- This is the case if $x$ is assigned in a \brf{randomised experiment}
- Or if $x$ is assigned in a \brf{quasi-experiment} that mimics random assignment
- Or if we can \brf{control for all confounders} in the analysis

\vfill

We should **always assume that ZCM is violated**. Researchers need to **think hard about confounders and how to eliminate them.** 


## Controlling for Confounders: Multivariate Regression

We can include \brf{confounders in the regression model} to control for them

$$y=\beta_0+\beta_1 x + \boldsymbol{S\gamma} + u$$
\vfill

Here, $\boldsymbol{S}$ is a vector of covariates $\boldsymbol{S}=(s_1', s_2', \dots, s_k')$ and $\boldsymbol{\gamma}=(\gamma_1, \gamma_2, \dots, \gamma_k)$ is a vector of coefficients,\footnote{Note: each element of $\boldsymbol{S}$ is in itself an $(1 \times n)$ vector. E.g. $s_1=(s_{11}, s_{12}, \dots, s_{1n})$ so $\boldsymbol{S}$ is actually an $(n \times k)$ matrix} i.e. 

$$\boldsymbol{S\gamma}=\gamma_1 s_1 + \gamma_2 s_2 + \dots + \gamma_k s_k$$
\vfill

We are \brf{only interested in $\beta_1$, the causal effect of $x$ on $y$}

- The other coefficients $\gamma_1, \gamma_2, \dots, \gamma_k$ are not of interest (nuisance parameters)
- We include the covariates $\boldsymbol{S}$ to control for confounders



## Interpretation of $\beta_1$ in Multivariate Regression

$\beta_1$ now has a \brf{ceteris paribus interpretation}

- **Holding all other variables $\boldsymbol{S}$ constant**, a one unit increase in $x$ leads to a $\beta_1$ unit increase in $y$

\vfill

The \brf{inclusion of $\boldsymbol{S}$} allows for a \brf{like-with-like comparison}

- We **compare units with the same values** of $\boldsymbol{S}$ but different values of $x$
- But the like-with like comparison is only valid if $\boldsymbol{S}$ contains all confounders



## Anatomy of Regression: the Frisch-Waugh-Lovell Theorem

Population model: 

$$y=\beta_0+\beta_1 x_1 + \beta_2 x_2  + u$$
\vfill
The FWL theorem states that three OLS estimators for $\beta_1$ are equivalent

1. regressing $y$ on $x_1$ and $x_2$
2. regressing $y$ on $\tilde{x}_1$, the residuals from a regression of $x_1$ on $x_2$
3. regressing $\tilde{y}$ on $\tilde{x}_1$, with $\tilde{y}$ being the residuals from a regression of $y$ on $x_2$

\vfill
The main insight comes from point 2...

## Anatomy of Regression: the Frisch-Waugh-Lovell Theorem

\brf{Multivariate regression does two steps at the same time}: 

1. It purges the correlation between $x_1$ and $x_2$ from $x_1$; the residuals $\tilde{x}_1$ are "what's left"; they are uncorrelated with $x_2$
2. It estimates the effect of $\tilde{x}_1$ on $y$, i.e. *after* purging the correlation between $x_1$ and $x_2$ 

\vfill

\brf{Why is this important?}

- If $x_2$ is a **confounder**, we can **purge** its influence by including it in the regression

\vfill
The same also holds for more than one control variable


## Example: Effect of Education on Earnings

```{r, eval=TRUE, echo=FALSE}
wage1 <- wage1 %>% mutate(abil=0.5*ed + rnorm(nrow(wage1), 0, 1))
```

```{r, eval=TRUE, echo=FALSE}
```



## Conditional Mean Independence Assumption

The \brf{Conditional Mean Independence Assumption (CMIA)} is a "light" version of the ZCM assumption. 

$$E(u \mid x, \boldsymbol{S})=E(u \mid S)=0$$
\vfill

In plain English: as long as \brf{$\boldsymbol{S}$ is included, the error term $u$ is uncorrelated with $x$}

- $x$ is exogenous conditional on $\boldsymbol{S}$
- $x$ is as good as random conditional on $\boldsymbol{S}$



## Controlled Regression in Practice




## Summary: What you need to understand for this module

\brf{Logic of linear regression}


- Why we use linear regression
- Why and how we use OLS to estimate the parameters of the linear regression model
- How to interpret the OLS estimator $\hat\beta_1$


\vfill

\brf{Limitations of linear regression for causal inference}

- The ZCM assumption is violated in most applications, leading to OVB
- How control variables can be used to control for confounders







## Appendix 


## Regression with R

```{r, eval=FALSE, echo=TRUE}
# Required packages (install if necessary)
library(tidyverse)
library(wooldridge)
library(stargazer)
```


## Regression with R

This code shows how to estimate and present regressions with R


```{r, eval=FALSE, echo=TRUE}
data('wage1') # load the data
df <- wage1
reg1 <- lm(wage ~ educ, data = df) # estimate simple regression
reg2 <- lm(wage ~ educ + exper, data = df) # estimate multivariate regression

stargazer(reg1, reg2, type = "text") # print regression results
```


## Regression outputs with Stargazer and R Markdown

You can generate nice regression tables with the `stargazer` package and R Markdown/Quarto. Here is a code chunk that gives you a nicely formatted latex table. 

````markdown
`r ''````{r, results='asis', echo=FALSE}
stargazer(reg1, reg2, 
      header=FALSE, 
      type='latex',
      title="Effect of Education on Wages")
```
````




## Regression with R

We can also generate a scatter with a regression line

```{r, eval=FALSE, echo=TRUE}
ggplot(df, aes(x = educ, y = wage)) + # generate scatter plot
  geom_point() + # add points
  geom_smooth(method = "lm", se = FALSE) +  # add regression line
  theme_minimal()
```

## Group Work Questions I

What is the difference between unbiasedness and consistency of an estimator?

\vfill

What is the sampling distribution of an estimator? Provide an intuitive explanation

\vfill

How is the sampling variance of an estimator related to the sample size? 


## Group Work Questions II

Suppose you have a variable $z$ that is omitted from the population model 

$$y=\beta_0 + \beta_1 x + \beta_2 z + u$$

The correct population parameter, $\beta_1$, is positive.

- a) Suppose you know that $cov(x, z)>0$ and $\beta_2<0$. How will the OLS estimator $\widehat\beta_1$ be biased when $z$ is omitted? Will it be over- or underestimated?
- b) Draw a DAG with $z$ as a confounder and indicate the two factors that make up the omitted variable bias 

## Group Work Questions III

```{r eval=TRUE, echo=FALSE}
datacal <- data.frame(read.dta("../../data/caschool.dta"))
reg1 <- lm(testscr ~ str, data=datacal)
reg2 <- lm(testscr ~ str + el_pct, data=datacal)
```

On the following slide, you see a regression output with two columns. In data on school districts in California, we want to study whether larger class sizes lead to lower test scores. The dependent variable is the average test score in a school district; the regressor of interest is the average class size in a school district. In Column (2), we control for the percentage of students in a school district who are not English native speakers (a proxy for poverty).

- a) Interpret the slope coefficient on class size in Column (1)
- b) Given the results in Column (2), what can we say about the correlation between class size and the share of non-native English speakers in a school district? 


## Group Work Questions III cont'd

\scriptsize
```{r, results='asis', echo=FALSE}
stargazer(reg1, reg2, 
          type="latex",
          header=FALSE, 
          no.space=TRUE,
          omit.stat=c("f", "ser"))
```
\normalsize

## .


```{r child = '../Templates/socialmedia.Rmd'}
```


## Contact

```{r child = '../Templates/contactpage.Rmd'}
```

